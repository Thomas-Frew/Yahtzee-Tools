{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 22:09:12.138094: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-26 22:09:12.139350: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 22:09:12.146739: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 22:09:12.158893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732621152.175845  724372 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732621152.181032  724372 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 22:09:12.200079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "\n",
    "\n",
    "import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"output_dataset.csv\"\n",
    "df = pd.read_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/personal-projects/Yahtzee-Tools/server/data_processing.py:54: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.random.rand() if pd.isna(x) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Twos</th>\n",
       "      <th>Threes</th>\n",
       "      <th>Fours</th>\n",
       "      <th>Fives</th>\n",
       "      <th>Sixes</th>\n",
       "      <th>Total</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>3 of a Kind</th>\n",
       "      <th>4 of a Kind</th>\n",
       "      <th>...</th>\n",
       "      <th>Total Mask</th>\n",
       "      <th>Bonus Mask</th>\n",
       "      <th>3 of a Kind Mask</th>\n",
       "      <th>4 of a Kind Mask</th>\n",
       "      <th>Full House Mask</th>\n",
       "      <th>Small Straight Mask</th>\n",
       "      <th>Large Straight Mask</th>\n",
       "      <th>Chance Mask</th>\n",
       "      <th>Yahtzee Mask</th>\n",
       "      <th>Score Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764849</td>\n",
       "      <td>0.098236</td>\n",
       "      <td>0.182197</td>\n",
       "      <td>0.729897</td>\n",
       "      <td>0.861138</td>\n",
       "      <td>0.702684</td>\n",
       "      <td>0.267828</td>\n",
       "      <td>0.462392</td>\n",
       "      <td>0.504530</td>\n",
       "      <td>0.625953</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335765</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.154620</td>\n",
       "      <td>0.237302</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.852621</td>\n",
       "      <td>0.243772</td>\n",
       "      <td>0.115154</td>\n",
       "      <td>0.085343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121841</td>\n",
       "      <td>0.479616</td>\n",
       "      <td>0.778028</td>\n",
       "      <td>0.200332</td>\n",
       "      <td>0.551002</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.296121</td>\n",
       "      <td>0.923280</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.912261</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.327652</td>\n",
       "      <td>0.357228</td>\n",
       "      <td>0.667100</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.186080</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.277276</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.752854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.485123</td>\n",
       "      <td>0.933361</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.206290</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.569239</td>\n",
       "      <td>0.716068</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.115904</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.619435</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.489671</td>\n",
       "      <td>0.933913</td>\n",
       "      <td>0.541004</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.040223</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.967810</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.489966</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.204752</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.167464</td>\n",
       "      <td>0.194301</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.903931</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.493386</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.472183</td>\n",
       "      <td>0.372947</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1529 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ones      Twos    Threes      Fours      Fives      Sixes  \\\n",
       "0     0.764849  0.098236  0.182197   0.729897   0.861138   0.702684   \n",
       "1     0.335765  0.204440  0.154620   0.237302   0.884010  18.000000   \n",
       "2     0.121841  0.479616  0.778028   0.200332   0.551002  18.000000   \n",
       "3     0.327652  0.357228  0.667100  12.000000   0.186080  18.000000   \n",
       "4     4.000000  0.485123  0.933361  12.000000   0.206290  18.000000   \n",
       "...        ...       ...       ...        ...        ...        ...   \n",
       "1524  2.000000  4.000000  0.619435  12.000000  20.000000  24.000000   \n",
       "1525  2.000000  4.000000  0.040223  12.000000  20.000000  24.000000   \n",
       "1526  2.000000  4.000000  0.204752  12.000000  20.000000  24.000000   \n",
       "1527  2.000000  4.000000  0.493386  12.000000  20.000000  24.000000   \n",
       "1528  2.000000  4.000000  9.000000  12.000000  20.000000  24.000000   \n",
       "\n",
       "          Total      Bonus  3 of a Kind  4 of a Kind  ...  Total Mask  \\\n",
       "0      0.267828   0.462392     0.504530     0.625953  ...         1.0   \n",
       "1      0.852621   0.243772     0.115154     0.085343  ...         1.0   \n",
       "2      0.296121   0.923280    25.000000     0.912261  ...         1.0   \n",
       "3      0.277276   0.266191    25.000000     0.752854  ...         1.0   \n",
       "4      0.569239   0.716068    25.000000     0.115904  ...         1.0   \n",
       "...         ...        ...          ...          ...  ...         ...   \n",
       "1524   0.489671   0.933913     0.541004     0.323255  ...         1.0   \n",
       "1525   0.967810   0.537200    24.000000     0.489966  ...         1.0   \n",
       "1526   0.167464   0.194301    24.000000     0.903931  ...         1.0   \n",
       "1527   0.472183   0.372947    24.000000    16.000000  ...         1.0   \n",
       "1528  71.000000  35.000000    24.000000    16.000000  ...         0.0   \n",
       "\n",
       "      Bonus Mask  3 of a Kind Mask  4 of a Kind Mask  Full House Mask  \\\n",
       "0            1.0               1.0               1.0              1.0   \n",
       "1            1.0               1.0               1.0              1.0   \n",
       "2            1.0               0.0               1.0              1.0   \n",
       "3            1.0               0.0               1.0              1.0   \n",
       "4            1.0               0.0               1.0              1.0   \n",
       "...          ...               ...               ...              ...   \n",
       "1524         1.0               1.0               1.0              0.0   \n",
       "1525         1.0               0.0               1.0              0.0   \n",
       "1526         1.0               0.0               1.0              0.0   \n",
       "1527         1.0               0.0               0.0              0.0   \n",
       "1528         0.0               0.0               0.0              0.0   \n",
       "\n",
       "      Small Straight Mask  Large Straight Mask  Chance Mask  Yahtzee Mask  \\\n",
       "0                     0.0                  1.0          1.0           1.0   \n",
       "1                     0.0                  1.0          1.0           1.0   \n",
       "2                     0.0                  1.0          1.0           1.0   \n",
       "3                     0.0                  1.0          1.0           1.0   \n",
       "4                     0.0                  1.0          1.0           1.0   \n",
       "...                   ...                  ...          ...           ...   \n",
       "1524                  1.0                  0.0          0.0           0.0   \n",
       "1525                  1.0                  0.0          0.0           0.0   \n",
       "1526                  0.0                  0.0          0.0           0.0   \n",
       "1527                  0.0                  0.0          0.0           0.0   \n",
       "1528                  0.0                  0.0          0.0           0.0   \n",
       "\n",
       "      Score Mask  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "1524         0.0  \n",
       "1525         0.0  \n",
       "1526         0.0  \n",
       "1527         0.0  \n",
       "1528         0.0  \n",
       "\n",
       "[1529 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process dataset\n",
    "df = dp.add_headers(df)\n",
    "df = dp.trim_id(df)\n",
    "df = dp.mask_data(df)\n",
    "df = dp.impute_nans(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       271\n",
       "1       271\n",
       "2       271\n",
       "3       271\n",
       "4       271\n",
       "       ... \n",
       "1524    312\n",
       "1525    312\n",
       "1526    312\n",
       "1527    312\n",
       "1528    312\n",
       "Name: Score, Length: 1529, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset\n",
    "X, y = dp.split_xy(df)\n",
    "X_train, X_test, y_train, y_test = dp.split_train_test(X, y, 0.2)\n",
    "\n",
    "# Normalize data\n",
    "X_train = dp.scale_data(X_train, True)\n",
    "X_test = dp.scale_data(X_test, False)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 22:09:14.563244: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "l2_reg = L2(0.01)  # Regularization factor (lambda)\n",
    "\n",
    "# Deep Neural Network\n",
    "model = Sequential([\n",
    "    InputLayer(shape=(31,)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2_reg),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2_reg),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2_reg),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 249.3739 - val_loss: 211.0728\n",
      "Epoch 2/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 194.7065 - val_loss: 79.2562\n",
      "Epoch 3/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8443 - val_loss: 55.2917\n",
      "Epoch 4/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.9030 - val_loss: 46.3960\n",
      "Epoch 5/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.4179 - val_loss: 40.6047\n",
      "Epoch 6/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.1017 - val_loss: 39.2186\n",
      "Epoch 7/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32.4499 - val_loss: 38.9822\n",
      "Epoch 8/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.9962 - val_loss: 37.5071\n",
      "Epoch 9/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.5650 - val_loss: 37.4580\n",
      "Epoch 10/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.9531 - val_loss: 37.6822\n",
      "Epoch 11/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.5250 - val_loss: 39.9974\n",
      "Epoch 12/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.7746 - val_loss: 38.1813\n",
      "Epoch 13/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28.7064 - val_loss: 35.3233\n",
      "Epoch 14/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.6671 - val_loss: 38.0432\n",
      "Epoch 15/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28.1613 - val_loss: 34.8721\n",
      "Epoch 16/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.3307 - val_loss: 38.0249\n",
      "Epoch 17/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.2833 - val_loss: 39.9025\n",
      "Epoch 18/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.3987 - val_loss: 38.1020\n",
      "Epoch 19/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.5549 - val_loss: 37.4479\n",
      "Epoch 20/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.0548 - val_loss: 36.3851\n",
      "Epoch 21/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26.1912 - val_loss: 37.1855\n",
      "Epoch 22/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.2381 - val_loss: 36.9566\n",
      "Epoch 23/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25.0250 - val_loss: 35.9772\n",
      "Epoch 24/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.9250 - val_loss: 36.7450\n",
      "Epoch 25/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.1924 - val_loss: 37.7834\n",
      "Epoch 26/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.4050 - val_loss: 36.9723\n",
      "Epoch 27/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.8652 - val_loss: 36.7492\n",
      "Epoch 28/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.9558 - val_loss: 36.9756\n",
      "Epoch 29/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.8632 - val_loss: 37.7073\n",
      "Epoch 30/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.3409 - val_loss: 38.5807\n",
      "Epoch 31/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.8970 - val_loss: 40.3001\n",
      "Epoch 32/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.4665 - val_loss: 36.7930\n",
      "Epoch 33/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.1838 - val_loss: 37.5324\n",
      "Epoch 34/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26.0889 - val_loss: 37.4318\n",
      "Epoch 35/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.0417 - val_loss: 37.0462\n",
      "Epoch 36/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.6346 - val_loss: 37.1076\n",
      "Epoch 37/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.1678 - val_loss: 37.4095\n",
      "Epoch 38/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.3819 - val_loss: 36.3737\n",
      "Epoch 39/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.6981 - val_loss: 38.1355\n",
      "Epoch 40/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.1632 - val_loss: 39.0927\n",
      "Epoch 41/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.1765 - val_loss: 37.9198\n",
      "Epoch 42/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.7209 - val_loss: 36.8044\n",
      "Epoch 43/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.9561 - val_loss: 37.2748\n",
      "Epoch 44/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.1921 - val_loss: 36.1856\n",
      "Epoch 45/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.0570 - val_loss: 37.2750\n",
      "Epoch 46/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.8865 - val_loss: 38.0775\n",
      "Epoch 47/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.0682 - val_loss: 36.8899\n",
      "Epoch 48/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.7136 - val_loss: 39.9247\n",
      "Epoch 49/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.9165 - val_loss: 38.3111\n",
      "Epoch 50/50\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.3005 - val_loss: 37.7902\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.9660\n",
      "Test Loss: 27.425853729248047\n"
     ]
    }
   ],
   "source": [
    "# Test loss\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Data:  [-0.3977190919278672, 0.8943287619643965, 1.5222345140446012, 0.8625911897103202, 0.8088059833494358, 0.7097742696907076, -0.42430754936254794, -0.3510774140818635, -0.9030054665623026, -0.6338940570193567, 0.753533830617963, 0.7310465055323538, 1.2209055739532544, 1.0395547845643076, 0.9002912738179939, 1.3953451013305738, -0.9025187307458946, -1.0581039908531071, -1.0409011951794773, -1.007386230857813, -0.962280755272859, 0.4487485326767854, 0.4487485326767854, 0.9466382788624977, -1.2106716344457786, -0.7219032335479355, -0.7284664228098997, -1.115363930404649, -0.9205607118322965, -1.1725004398947565, 0.0]\n",
      "Prediction:  279.35977\n",
      "Actual:  292\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Make predictions on the test data, print random\n",
    "random_index = random.randint(0, len(X_test) - 1)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Data: \", list(X_test[random_index]))\n",
    "print(\"Prediction: \", predictions[random_index][0]) \n",
    "print(\"Actual: \", y_test.iloc[random_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/personal-projects/Yahtzee-Tools/server/data_processing.py:54: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.random.rand() if pd.isna(x) else x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266.0704"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import nan\n",
    "\n",
    "# Custom tests\n",
    "x1 = nan;   x2 = nan;   x3 = nan\n",
    "x4 = nan;   x5 = nan;   x6 = nan\n",
    "tt = nan;   bn = nan;   k3 = nan\n",
    "k4 = nan;   fh = nan;   ss = nan\n",
    "ls = nan;   ch = nan;   yz = nan\n",
    "\n",
    "sample = [\"Sample\",x1,x2,x3,x4,x5,x6,tt,bn,k3,k4,fh,ss,ls,ch,yz,0]\n",
    "sample_df = pd.DataFrame([sample])\n",
    "sample_df = dp.add_headers(sample_df)\n",
    "sample_df = dp.trim_id(sample_df)\n",
    "sample_df = dp.mask_data(sample_df)\n",
    "sample_df = dp.impute_nans(sample_df)\n",
    "sample_df, _ = dp.split_xy(sample_df)\n",
    "sample_df = dp.scale_data(sample_df, False)\n",
    "\n",
    "model.predict(sample_df)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_filename = \"output_model.keras\"\n",
    "model.save(h5_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
